{"cells":[{"cell_type":"markdown","metadata":{"id":"fIRXlmg0yGK8"},"source":["# Last.fm ETL Pipeline\n","\n","This notebook implements an Extract, Transform, Load (ETL) pipeline for Last.fm listening data.\n","It collects data from the Last.fm API and stores it in a structured SQLite database for analysis.\n","\n","## Overview\n","- **Extract**: Pull data from Last.fm API (listening history, artists, albums, tracks, tags)\n","- **Transform**: Normalize data and prepare relationships\n","- **Load**: Store in a relational database with proper schema\n","- **Analyze**: Query the database for previews of created tables\n","\n"]},{"cell_type":"markdown","source":["## Configuration & Dependencies\n","\n","This section sets up the necessary libraries, API credentials, and configuration parameters. If required, change configuration constants (recent_tracks_limit, top_items_limit,recent_pages,time_periods) in the `config.py` file.\n"],"metadata":{"id":"vmHc2nk-40km"}},{"cell_type":"code","source":["# Standard library imports\n","import os\n","import sys\n","import json\n","import time\n","import logging\n","from datetime import datetime\n","\n","# Third-party imports\n","import importlib\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import requests\n","import seaborn as sns\n","import sqlite3\n","from tqdm.notebook import tqdm\n","from google.colab import drive"],"metadata":{"id":"ieoqeX0wYG8Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HCpSDYGiyGLA"},"outputs":[],"source":["# Mount Google Drive (must happen before path setup)\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Path setup\n","drive_path = \"/content/drive/MyDrive/Colab-Notebooks/last-fm-data\"  # **REPLACE WITH YOUR ACTUAL PATH**\n","os.makedirs(drive_path, exist_ok=True)  # Create the directory if it doesn't exist\n","print(f\"Drive path: {drive_path}\")\n","\n","# Create a data subfolder for database and logs\n","data_path = os.path.join(drive_path, \"data\")\n","os.makedirs(data_path, exist_ok=True)  # Create the data directory if it doesn't exist\n","print(f\"Data path: {data_path}\")\n","\n","# Create config folder if it doesn't exist\n","config_path = os.path.join(drive_path, \"config\")\n","os.makedirs(config_path, exist_ok=True)\n","print(f\"Config path: {config_path}\")\n","\n","# Add drive_path to system path for module imports\n","sys.path.append(drive_path)\n","\n","# Set a default value for BACKUP_TO_DRIVE\n","BACKUP_TO_DRIVE = True"]},{"cell_type":"code","source":["# Define file paths\n","DB_PATH = os.path.join(data_path, \"lastfm_data.db\")\n","LOGS_PATH = os.path.join(data_path, \"lastfm_etl.log\")\n","SCHEMA_PATH = os.path.join(config_path, \"schema.sql\")\n","\n","# Local module imports - must come after path setup\n","try:\n","    from utils.lastfm_api import LastFMAPI\n","    from utils.data_collector import DataCollector\n","    from utils.database_helper import DatabaseHelper\n","    # import lastfm username and API key\n","    from config.config import LASTFM_API_KEY, USERNAME, COLLECTION_SETTINGS\n","    print(\"Successfully imported all required modules\")\n","except ImportError as e:\n","    print(f\"Error importing modules: {e}\")\n","    print(\"Make sure the python files are in the correct location!\")"],"metadata":{"id":"MTy1xjAehYKe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up logging\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s',\n","    force=True,  #  Add force=True\n","    handlers=[\n","        logging.FileHandler(LOGS_PATH),\n","        logging.StreamHandler(sys.stdout)\n","    ]\n",")\n","logger = logging.getLogger('lastfm_etl')\n","\n","logger.info(\"Logger is now active!\")"],"metadata":{"id":"q7Wj-ay-eJAk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to recreate the database with the updated schema\n","def recreate_database():\n","    \"\"\"Recreate the database with the updated schema.\"\"\"\n","    logger.info(\"Recreating database with updated schema...\")\n","\n","    # Check if database exists and create a backup if it does\n","    if os.path.exists(DB_PATH):\n","        # Create a backup with timestamp\n","        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","        backup_path = os.path.join(data_path, f\"lastfm_data_backup_{timestamp}.db\")\n","\n","        # Copy the existing database to backup\n","        try:\n","            import shutil\n","            shutil.copy2(DB_PATH, backup_path)\n","            logger.info(f\"Created backup of existing database at {backup_path}\")\n","        except Exception as e:\n","            logger.warning(f\"Could not create database backup: {e}\")\n","\n","        # Delete the existing database\n","        try:\n","            os.remove(DB_PATH)\n","            logger.info(f\"Deleted existing database: {DB_PATH}\")\n","        except Exception as e:\n","            logger.error(f\"Failed to delete existing database: {e}\")\n","            return False\n","\n","    # Initialize database with the updated schema\n","    db_helper = DatabaseHelper(DB_PATH)\n","    success = db_helper.initialize_database(schema_file=SCHEMA_PATH)\n","\n","    if success:\n","        logger.info(\"Database successfully recreated with the new schema!\")\n","    else:\n","        logger.error(\"Failed to recreate database.\")\n","\n","    return success\n","\n","# Ask the user if they want to recreate the database\n","recreate_db = input(\"Do you want to recreate the database with the updated schema? (y/n): \")\n","if recreate_db.lower() == 'y':\n","    recreate_database()"],"metadata":{"id":"JwVCLKioazGe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def backup_to_drive():\n","    \"\"\"Backup database and logs to Google Drive\"\"\"\n","    if not BACKUP_TO_DRIVE:\n","        return\n","\n","    try:\n","        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","\n","        # Copy database\n","        if os.path.exists(DB_PATH):\n","            backup_db_path = os.path.join(data_path, f\"data_{timestamp}.db\")\n","            os.system(f\"cp {DB_PATH} '{backup_db_path}'\")\n","            logger.info(f\"Database backed up to: {backup_db_path}\")\n","\n","        # Copy logs\n","        if os.path.exists(LOGS_PATH):\n","            backup_log_path = os.path.join(data_path, f\"lastfm_etl_{timestamp}.log\")\n","            os.system(f\"cp {LOGS_PATH} '{backup_log_path}'\")\n","            logger.info(f\"Logs backed up to: {backup_log_path}\")\n","\n","    except Exception as e:\n","        logger.error(f\"Failed to backup files to Google Drive: {e}\")"],"metadata":{"id":"F2HRfrSAwM9r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pipeline Execution\n","\n","Running the complete ETL pipeline to collect Last.fm data."],"metadata":{"id":"eFrm2BGr5X_a"}},{"cell_type":"code","source":["def run_etl_pipeline():\n","    \"\"\"Run the complete ETL pipeline\"\"\"\n","    start_time = time.time()\n","    logger.info(\"Starting Last.fm ETL pipeline\")\n","\n","    try:\n","        # Initialize API and collector\n","        lastfm_api = LastFMAPI(LASTFM_API_KEY)\n","        collector = DataCollector(lastfm_api, USERNAME)\n","        db_helper = DatabaseHelper(DB_PATH)\n","        print(db_helper)  # Print the object\n","        print(type(db_helper))\n","\n","        # Initialize database with schema\n","        logger.info(\"Initializing database...\")\n","        db_helper.initialize_database(schema_file=SCHEMA_PATH)\n","\n","        # Collect data from Last.fm API\n","        logger.info(\"Collecting data from Last.fm API...\")\n","        collected_data = collector.collect_library_data(\n","            recent_tracks_limit=COLLECTION_SETTINGS['recent_tracks_limit'],\n","            top_items_limit=COLLECTION_SETTINGS['top_items_limit'],\n","            recent_pages=COLLECTION_SETTINGS['recent_pages'],\n","            time_periods=COLLECTION_SETTINGS['time_periods']\n","        )\n","        #  Enhanced Debugging: Inspect collected_data\n","        logger.info(\"--- Debugging collected_data ---\")\n","        for key, value in collected_data.items():\n","            logger.info(f\"Key: {key}, Type: {type(value)}, Length: {len(value) if isinstance(value, list) else 'N/A'}\")\n","            if key in ['albums', 'tracks']:  #  Focus on albums and tracks\n","                for item in value[:2]:  #  Print the first 2 items as examples\n","                    logger.info(f\"  Example {key[:-1]}: {item}\")\n","\n","\n","        # Process and load data into database\n","        logger.info(\"Inserting collected data into database...\")\n","        stats = db_helper.process_collected_data(collected_data)\n","\n","        if stats:\n","            logger.info(\"ETL pipeline completed successfully!\")\n","            logger.info(f\"Inserted: {stats['artists']} artists, {stats['albums']} albums, \"\n","                        f\"{stats['tracks']} tracks, {stats['tags']} tags, \"\n","                        f\"{stats['history_items']} listening history records\")\n","        else:\n","            logger.error(\"Failed to process data\")\n","\n","        # Backup to Google Drive\n","        if BACKUP_TO_DRIVE:\n","            backup_to_drive()\n","\n","        # Calculate runtime\n","        runtime = time.time() - start_time\n","        logger.info(f\"Total runtime: {runtime:.2f} seconds ({runtime/60:.2f} minutes)\")\n","        return True\n","\n","    except Exception as e:\n","        logger.error(f\"ETL pipeline failed: {e}\")\n","        # Try to backup any data collected to this point\n","        try:\n","            if 'collected_data' in locals():\n","                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","                error_data_path = os.path.join(data_path, f\"failed_run_data_{timestamp}.json\")\n","                with open(error_data_path, 'w') as f:\n","                    json.dump(collected_data, f)\n","                logger.info(f\"Partial data saved to {error_data_path}\")\n","        except Exception as backup_error:\n","            logger.error(f\"Failed to save partial data: {backup_error}\")\n","        return False\n","    finally: # Ensure this block runs even if there's an error\n","        # Close the handlers\n","        for handler in logger.handlers[:]:  # Iterate over a copy of the handlers list\n","            handler.close()\n","            logger.removeHandler(handler)  # Clean up the logger as well"],"metadata":{"id":"Mw6NElpwwUT3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*60)\n","print(\"       LAST.FM DATA ETL PIPELINE - STARTING EXECUTION       \")\n","print(\"=\"*60 + \"\\n\")\n","\n","# Check if all required classes are defined\n","required_classes = ['LastFMAPI', 'DataCollector', 'DatabaseHelper']\n","missing_classes = [cls for cls in required_classes if cls not in globals()]\n","\n","if missing_classes:\n","    print(f\"ERROR: Missing required class definitions: {', '.join(missing_classes)}\")\n","    print(\"Please make sure you've run the cells defining these classes first.\")\n","else:\n","    # Execute the ETL pipeline\n","    success = run_etl_pipeline()\n","\n","    # Show completion message\n","    print(\"\\n\" + \"=\"*60)\n","    if success:\n","        print(\"       ETL PIPELINE EXECUTION COMPLETED SUCCESSFULLY       \")\n","    else:\n","        print(\"       ETL PIPELINE EXECUTION COMPLETED WITH ERRORS       \")\n","    print(\"=\"*60 + \"\\n\")"],"metadata":{"id":"hQ0exTggB1ba"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Analysis Examples\n","\n","Just some code to check format of the tables."],"metadata":{"id":"jaJRnIqh5evt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"l3GgBUUMyGLF"},"outputs":[],"source":["# Connect to the database\n","conn = sqlite3.connect(DB_PATH)\n","cursor = conn.cursor()\n","\n","# Check which tables exist in the database\n","cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n","tables = cursor.fetchall()\n","print(\"Available tables in the database:\")\n","table_names = []\n","for table in tables:\n","    table_name = table[0]\n","    table_names.append(table_name)\n","    print(f\"- {table_name}\")\n","\n","# Print the first 5 rows of each table\n","print(\"\\n\" + \"=\"*50)\n","print(\"TABLE PREVIEWS\")\n","print(\"=\"*50)\n","\n","for table_name in table_names:\n","    try:\n","        # Get column names\n","        cursor.execute(f\"PRAGMA table_info({table_name})\")\n","        columns = [col[1] for col in cursor.fetchall()]\n","\n","        # Get first 5 rows\n","        query = f\"SELECT * FROM {table_name} LIMIT 5\"\n","        df = pd.read_sql_query(query, conn)\n","\n","        print(f\"\\n{'-'*50}\")\n","        print(f\"TABLE: {table_name}\")\n","        print(f\"Columns: {', '.join(columns)}\")\n","        print(f\"Row count: {pd.read_sql_query(f'SELECT COUNT(*) FROM {table_name}', conn).iloc[0, 0]}\")\n","        print(f\"{'-'*50}\")\n","\n","        if not df.empty:\n","            print(df.head())\n","        else:\n","            print(\"(Table is empty)\")\n","    except Exception as e:\n","        print(f\"Error reading table {table_name}: {e}\")\n","\n","# Close the connection\n","conn.close()\n","print(\"\\nDatabase connection closed.\")"]},{"cell_type":"code","source":["import sqlite3\n","import pandas as pd\n","import os\n","\n","# Define your database path\n","DB_PATH = \"/content/drive/MyDrive/Colab-Notebooks/last-fm-data/data/lastfm_data.db\"\n","\n","def analyze_database():\n","    \"\"\"Analyze albums and tracks tables to identify missing information.\"\"\"\n","    conn = sqlite3.connect(DB_PATH)\n","\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"DATABASE ANALYSIS - ALBUMS AND TRACKS\")\n","    print(\"=\"*80)\n","\n","    # Get all albums\n","    print(\"\\n--- ALBUMS TABLE ---\")\n","    albums_df = pd.read_sql_query(\"SELECT * FROM albums\", conn)\n","    print(f\"Total albums: {len(albums_df)}\")\n","\n","    # Display all albums\n","    if len(albums_df) > 0:\n","        # Count null values in each column\n","        null_counts = albums_df.isnull().sum()\n","        print(\"\\nNull value counts by column:\")\n","        for column, count in null_counts.items():\n","            percentage = (count / len(albums_df)) * 100\n","            print(f\"  {column}: {count} nulls ({percentage:.1f}%)\")\n","\n","        # Show first 10 albums\n","        print(\"\\nFirst 10 albums:\")\n","        print(albums_df.head(10))\n","    else:\n","        print(\"No albums found in the database!\")\n","\n","    # Get all tracks\n","    print(\"\\n\\n--- TRACKS TABLE ---\")\n","    tracks_df = pd.read_sql_query(\"SELECT * FROM tracks\", conn)\n","    print(f\"Total tracks: {len(tracks_df)}\")\n","\n","    # Display all tracks\n","    if len(tracks_df) > 0:\n","        # Count null values in each column\n","        null_counts = tracks_df.isnull().sum()\n","        print(\"\\nNull value counts by column:\")\n","        for column, count in null_counts.items():\n","            percentage = (count / len(tracks_df)) * 100\n","            print(f\"  {column}: {count} nulls ({percentage:.1f}%)\")\n","\n","        # Count tracks without album_id\n","        tracks_without_album = tracks_df[tracks_df['album_id'].isnull()]\n","        print(f\"\\nTracks without album_id: {len(tracks_without_album)} ({len(tracks_without_album)/len(tracks_df)*100:.1f}%)\")\n","\n","        # Show first 10 tracks\n","        print(\"\\nFirst 10 tracks:\")\n","        print(tracks_df.head(10))\n","\n","        # Show a few tracks without album_id\n","        if len(tracks_without_album) > 0:\n","            print(\"\\nSample of tracks without album_id:\")\n","            print(tracks_without_album.head(5))\n","\n","        # Get artist names for tracks\n","        print(\"\\nJoining artist names to tracks:\")\n","        tracks_with_artists = pd.read_sql_query(\"\"\"\n","            SELECT t.track_id, t.name AS track_name, t.album_id,\n","                   a.name AS artist_name, t.url\n","            FROM tracks t\n","            JOIN artists a ON t.artist_id = a.artist_id\n","            LIMIT 10\n","        \"\"\", conn)\n","        print(tracks_with_artists)\n","\n","        # Get album names for tracks that have album_id\n","        print(\"\\nJoining album names to tracks (where album_id exists):\")\n","        tracks_with_albums = pd.read_sql_query(\"\"\"\n","            SELECT t.track_id, t.name AS track_name,\n","                   a.name AS artist_name,\n","                   al.name AS album_name, t.url\n","            FROM tracks t\n","            JOIN artists a ON t.artist_id = a.artist_id\n","            LEFT JOIN albums al ON t.album_id = al.album_id\n","            WHERE t.album_id IS NOT NULL\n","            LIMIT 10\n","        \"\"\", conn)\n","        print(tracks_with_albums)\n","    else:\n","        print(\"No tracks found in the database!\")\n","\n","    # Analysis - Count tracks per artist\n","    print(\"\\n\\n--- ANALYSIS: TRACKS PER ARTIST ---\")\n","    tracks_per_artist = pd.read_sql_query(\"\"\"\n","        SELECT a.name AS artist_name, COUNT(t.track_id) AS track_count\n","        FROM artists a\n","        LEFT JOIN tracks t ON a.artist_id = t.artist_id\n","        GROUP BY a.artist_id\n","        ORDER BY track_count DESC\n","    \"\"\", conn)\n","    print(tracks_per_artist.head(10))\n","\n","    # Analysis - Count albums per artist\n","    print(\"\\n--- ANALYSIS: ALBUMS PER ARTIST ---\")\n","    albums_per_artist = pd.read_sql_query(\"\"\"\n","        SELECT a.name AS artist_name, COUNT(al.album_id) AS album_count\n","        FROM artists a\n","        LEFT JOIN albums al ON a.artist_id = al.artist_id\n","        GROUP BY a.artist_id\n","        ORDER BY album_count DESC\n","    \"\"\", conn)\n","    print(albums_per_artist.head(10))\n","\n","    # Close connection\n","    conn.close()\n","    print(\"\\nDatabase connection closed.\")\n","\n","# Execute the analysis\n","analyze_database()"],"metadata":{"id":"f4VumajI48l5"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"colab":{"provenance":[{"file_id":"1VRCOIvhv7gn_1U-XrsUVxvK1WgNg4qKA","timestamp":1739978535572}]}},"nbformat":4,"nbformat_minor":0}